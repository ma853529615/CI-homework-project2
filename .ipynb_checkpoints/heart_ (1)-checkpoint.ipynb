{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "import cvxopt.solvers\n",
    "import numpy.linalg as la\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chromosome:\n",
    " \n",
    "    def __init__(self,  vardim, bound):\n",
    "\n",
    "        self.vardim = vardim\n",
    "        self.bound = bound\n",
    "        self.fitness = 0.\n",
    " \n",
    "    def generate(self):\n",
    "\n",
    "        len = self.vardim\n",
    "        rnd1 = np.random.random(size=len)-0.5\n",
    "        rnd2 = np.random.random(size=len)-0.5\n",
    "        self.chrom = np.zeros(len)\n",
    "        self.cig = np.zeros(len)\n",
    "        for i in range(0, len):\n",
    "            self.chrom[i] = self.bound[0, i] + \\\n",
    "                (self.bound[1, i] - self.bound[0, i]) * rnd1[i]\n",
    "            self.cig[i] = self.bound[0, i] + \\\n",
    "                (self.bound[1, i] - self.bound[0, i]) * rnd2[i]\n",
    "        #print(\"chorm: {}, \".format(self.chrom)+\"cigmal: {}\".format(self.cig))\n",
    "    def calculateFitness(self,dataset):\n",
    "        \n",
    "        #print(self.chrom)\n",
    "        self.fitness = SVMResult(\n",
    "            self.vardim, self.chrom, self.bound, dataset)\n",
    "        \n",
    "    def print_(self):\n",
    "        \n",
    "        print(self.chrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GA:\n",
    " \n",
    "    def __init__(self, sizepop, vardim, bound, MAXGEN, params, k):\n",
    "        \n",
    "        self.remain = sizepop\n",
    "        self.realsize = self.remain\n",
    "        self.sizepop = self.remain*k\n",
    "        self.MAXGEN = MAXGEN\n",
    "        self.vardim = vardim\n",
    "        self.bound = bound\n",
    "        self.k = k\n",
    "        self.population = []\n",
    "        self.fitness = np.zeros((self.sizepop, 1))\n",
    "        self.trace = np.zeros((self.MAXGEN, 2))\n",
    "        self.params = params\n",
    "        self.dataset = self.creat_dataset(pd.read_csv(\"diabetes.csv\"))\n",
    "        self.lr = 1\n",
    "        \n",
    "    def creat_dataset(self, df):\n",
    "        \n",
    "        df = shuffle(df)\n",
    "        size = len(df)\n",
    "        df['split'] = 0\n",
    "        df.iloc[0:math.ceil(0.3*size),-1] = 'train'\n",
    "        #df.iloc[math.ceil(0.7*size):math.ceil(0.85*size), -1] = 'test'\n",
    "        df.iloc[math.ceil(0.7*size):size, -1] = 'val' #调高验证集比例\n",
    "\n",
    "        return df\n",
    " \n",
    "    def initialize(self):\n",
    "        \n",
    "        for i in range(0, self.remain):\n",
    "            ind = Chromosome(self.vardim, self.bound)\n",
    "            ind.generate()\n",
    "            self.population = np.append(self.population, ind)\n",
    "        self.population = np.array(self.population) \n",
    "        \n",
    "    def evaluate(self):\n",
    "        \n",
    "        fitness = []\n",
    "        for i in tqdm_notebook(range(0, self.realsize)):\n",
    "            self.population[i].calculateFitness(self.dataset)\n",
    "            #print(\"###/n\")\n",
    "            fitness.append(self.population[i].fitness)\n",
    "        self.fitness = np.array(fitness)\n",
    "        best_idx = np.argmax(self.fitness)\n",
    "        self.best_score = self.fitness[best_idx]\n",
    "        self.best = self.population[best_idx]\n",
    "        \n",
    "    def show(self):\n",
    "        \n",
    "        for i in range(0, self.realsize):\n",
    "            #print('chorme {}: '.format(i))\n",
    "            self.population[i].print_()\n",
    "            print(self.population[i].fitness)\n",
    " \n",
    "    def solve(self):\n",
    "        \n",
    "        self.t = 0\n",
    "        self.initialize()\n",
    "        self.evaluate()\n",
    "        #best = np.max(self.fitness)\n",
    "        #bestIndex = np.argmax(self.fitness)\n",
    "        #self.best = copy.deepcopy(self.population[bestIndex])\n",
    "        #self.avefitness = np.mean(self.fitness)\n",
    "        #self.trace[self.t, 0] = self.best.fitness\n",
    "        #self.trace[self.t, 1] = self.avefitness\n",
    "        #self.trace[self.t, 0] = (1 - self.best.fitness) / self.best.fitness\n",
    "        #self.trace[self.t, 1] = (1 - self.avefitness) / self.avefitness\n",
    "        print(\"本代最好的染色体: {}\".format(self.best.chrom))\n",
    "        print(\"本代最高分: {}\".format(self.best_score))\n",
    "        #print(\"第%d代: 最高分: %f; 平均分%f\" % (\n",
    "            #self.t, self.trace[self.t, 0], self.trace[self.t, 1]))\n",
    "        while (self.t < self.MAXGEN - 1):\n",
    "            self.t += 1\n",
    "            self.crossover()\n",
    "            self.mutation()\n",
    "            self.evaluate()\n",
    "            self.selection()\n",
    "            \n",
    "            #print(\"best? {}\".format(self.population[-1].chrom))\n",
    "            #self.show()\n",
    "            best = np.max(self.fitness)\n",
    "            bestIndex = np.argmax(self.fitness)\n",
    "            #print(\"本代最好的染色体: {}\".format(self.population[bestIndex].chrom))\n",
    "            #print(\"本代最高分: {}\".format(self.population[bestIndex].fitness))\n",
    "            #if best > self.best.fitness:#改动\n",
    "            #self.best = copy.deepcopy(self.population[bestIndex])\n",
    "            #self.avefitness = np.mean(self.fitness)\n",
    "            #self.trace[self.t, 0] = self.best.fitness\n",
    "            #self.trace[self.t, 1] = self.avefitness\n",
    "            #self.trace[self.t, 0] = (1 - self.best.fitness) / self.best.fitness\n",
    "            #self.trace[self.t, 1] = (1 - self.avefitness) / self.avefitness\n",
    "            \n",
    "            #print(\"第%d代: 最高分: %f; 平均分%f\" % (\n",
    "                #self.t, self.trace[self.t, 0], self.trace[self.t, 1]))\n",
    "            print(\"本代最好的染色体: {}\".format(self.population[bestIndex].chrom))\n",
    "            print(\"本代最高分: {}\".format(self.population[bestIndex].fitness))\n",
    " \n",
    "        print(\"Optimal function value is: %f; \" %\n",
    "              self.trace[self.t, 0])\n",
    "        print(\"Optimal solution is:\")\n",
    "        print(self.best.chrom)\n",
    "        self.printResult()\n",
    " \n",
    "    def selection(self):\n",
    "        \n",
    "        #硬选择\n",
    "        #print(\"max: {}\".format(np.argmax(self.fitness)))\n",
    "        #print(self.fitness[-1])\n",
    "        sort_idx = np.argsort(self.fitness, axis=0).reshape(1,-1)[0]  #????\n",
    "        top_idx = sort_idx[len(self.fitness)-self.remain-1:-1]\n",
    "        #print(sort_idx[0:self.remain])\n",
    "        new_fitness = copy.deepcopy(self.fitness[top_idx])\n",
    "        self.fitness = new_fitness\n",
    "        #print(len(self.fitness))\n",
    "        new = copy.deepcopy(self.population[top_idx]) #深复制\n",
    "        self.population  = new\n",
    "        self.realsize = len(self.population)\n",
    "        #print(self.realsize)\n",
    "    \n",
    "    \"\"\"\n",
    "     def selection(self):\n",
    "     \n",
    "        #软选择\n",
    "        #print(\"max: {}\".format(np.argmax(self.fitness)))\n",
    "        #print(self.fitness)\n",
    "        sort_idx = np.argsort(self.fitness, axis=0).reshape(1,-1)[0]\n",
    "        #print(sort_idx[0:self.remain])\n",
    "        new = copy.deepcopy(self.population[sort_idx[0:self.remain]])\n",
    "        self.population  = new\n",
    "        self.realsize = len(self.population)\n",
    "        #print(self.realsize)\n",
    "    \"\"\"\n",
    "    \n",
    "    def crossover(self):\n",
    "        \n",
    "        newpop = []\n",
    "       # print(self.sizepop-self.remain)\n",
    "        #print(len(self.population)) \n",
    "        #print(self.sizepop-self.remain-10)\n",
    "        for i in range(self.sizepop-self.remain-10):\n",
    "            idx1 = random.randint(0, self.remain - 1)\n",
    "            idx2 = random.randint(0, self.remain - 1)\n",
    "            while idx2 == idx1:\n",
    "                idx2 = random.randint(0, self.remain - 1)\n",
    "            new = copy.deepcopy(self.population[idx1])\n",
    "            new.chrom += self.population[idx2].chrom\n",
    "            new.chrom /= 2\n",
    "            new.cig += self.population[idx2].cig\n",
    "            new.cig /= 2\n",
    "            self.population = np.append(self.population, new)\n",
    "        #print(len(self.population))\n",
    "        for i in range(9):\n",
    "            new = Chromosome(self.vardim, self.bound)\n",
    "            new.generate()\n",
    "            self.population = np.append(self.population, new)\n",
    "        self.realsize = len(self.population)\n",
    "        #print(self.realsize)\n",
    "        \n",
    "    def mutation(self):\n",
    "        \n",
    "        newpop = []\n",
    "        self.lr *= 0.9\n",
    "        for i in range(0, self.sizepop-1):\n",
    "            p = random.random()-0.5\n",
    "            newpop.append(copy.deepcopy(self.population[i]))\n",
    "            rand = random.random()-0.5\n",
    "            for j in range(0, self.vardim):\n",
    "                #s = 0\n",
    "                #while newpop[i].chrom[j]<=self.bound[0][j] or newpop[i].chrom[j]>=self.bound[1][j]: #限制bound\n",
    "                    \n",
    "                #s+=1\n",
    "                randn = random.random()-0.5\n",
    "                flag = 1\n",
    "                while newpop[i].cig[j]==0 or flag==1:\n",
    "                    flag = 0\n",
    "                    newpop[i].cig[j] = newpop[i].cig[j]*np.exp(rand+randn)\n",
    "\n",
    "                newpop[i].chrom[j] = newpop[i].cig[j]+newpop[i].cig[j]*randn*self.lr #增加学习率\n",
    "        newpop.append(self.best)    #保留最佳    \n",
    "                #print(s)\n",
    "        self.population = np.array(newpop)\n",
    "        self.realsize = len(self.population)\n",
    "        #print(self.realsize)\n",
    " \n",
    "    def printResult(self):\n",
    "        \n",
    "        '''\n",
    "        plot the result of the genetic algorithm\n",
    "        '''\n",
    "        x = np.arange(0, self.MAXGEN)\n",
    "        y1 = self.trace[:, 0]\n",
    "        y2 = self.trace[:, 1]\n",
    "        plt.plot(x, y1, 'r', label='optimal value')\n",
    "        plt.plot(x, y2, 'g', label='average value')\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"function value\")\n",
    "        plt.title(\"Genetic algorithm for function optimization\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SUPPORT_VECTOR_MULTIPLIER = 1e-5\n",
    "\n",
    "#https://github.com/ajtulloch/svmpy下载的\n",
    "class SVMTrainer(object):\n",
    "    def __init__(self, kernel, c):\n",
    "        self._kernel = kernel\n",
    "        self._c = c\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Given the training features X with labels y, returns a SVM\n",
    "        predictor representing the trained SVM.\n",
    "        \"\"\"\n",
    "        lagrange_multipliers = self._compute_multipliers(X, y)\n",
    "        return self._construct_predictor(X, y, lagrange_multipliers)\n",
    "\n",
    "    def _gram_matrix(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        # TODO(tulloch) - vectorize\n",
    "        for i, x_i in enumerate(X):\n",
    "            for j, x_j in enumerate(X):\n",
    "                K[i, j] = self._kernel(x_i, x_j)\n",
    "        return K\n",
    "\n",
    "    def _construct_predictor(self, X, y, lagrange_multipliers):\n",
    "        support_vector_indices = \\\n",
    "            lagrange_multipliers > MIN_SUPPORT_VECTOR_MULTIPLIER\n",
    "\n",
    "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
    "        support_vectors = X[support_vector_indices]\n",
    "        support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        # http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/kernels.pdf\n",
    "        # bias = y_k - \\sum z_i y_i  K(x_k, x_i)\n",
    "        # Thus we can just predict an example with bias of zero, and\n",
    "        # compute error.\n",
    "        bias = np.mean(\n",
    "            [y_k - SVMPredictor(\n",
    "                kernel=self._kernel,\n",
    "                bias=0.0,\n",
    "                weights=support_multipliers,\n",
    "                support_vectors=support_vectors,\n",
    "                support_vector_labels=support_vector_labels).predict(x_k)\n",
    "             for (y_k, x_k) in zip(support_vector_labels, support_vectors)])\n",
    "\n",
    "        return SVMPredictor(\n",
    "            kernel=self._kernel,\n",
    "            bias=bias,\n",
    "            weights=support_multipliers,\n",
    "            support_vectors=support_vectors,\n",
    "            support_vector_labels=support_vector_labels)\n",
    "\n",
    "    def _compute_multipliers(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        K = self._gram_matrix(X)\n",
    "        # Solves\n",
    "        # min 1/2 x^T P x + q^T x\n",
    "        # s.t.\n",
    "        #  Gx \\coneleq h\n",
    "        #  Ax = b\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y, y) * K)\n",
    "        q = cvxopt.matrix(-1 * np.ones(n_samples))\n",
    "\n",
    "        # -a_i \\leq 0\n",
    "        # TODO(tulloch) - modify G, h so that we have a soft-margin classifier\n",
    "        G_std = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "        h_std = cvxopt.matrix(np.zeros(n_samples))\n",
    "\n",
    "        # a_i \\leq c\n",
    "        G_slack = cvxopt.matrix(np.diag(np.ones(n_samples)))\n",
    "        h_slack = cvxopt.matrix(np.ones(n_samples) * self._c)\n",
    "\n",
    "        G = cvxopt.matrix(np.vstack((G_std, G_slack)))\n",
    "        h = cvxopt.matrix(np.vstack((h_std, h_slack)))\n",
    "        A = cvxopt.matrix(np.double(y), (1, n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        \n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        return np.ravel(solution['x'])\n",
    "\n",
    "\n",
    "class SVMPredictor(object):\n",
    "    def __init__(self,\n",
    "                 kernel,\n",
    "                 bias,\n",
    "                 weights,\n",
    "                 support_vectors,\n",
    "                 support_vector_labels):\n",
    "        self._kernel = kernel\n",
    "        self._bias = bias\n",
    "        self._weights = weights\n",
    "        self._support_vectors = support_vectors\n",
    "        self._support_vector_labels = support_vector_labels\n",
    "        assert len(support_vectors) == len(support_vector_labels)\n",
    "        assert len(weights) == len(support_vector_labels)\n",
    "        logging.info(\"Bias: %s\", self._bias)\n",
    "        logging.info(\"Weights: %s\", self._weights)\n",
    "        logging.info(\"Support vectors: %s\", self._support_vectors)\n",
    "        logging.info(\"Support vector labels: %s\", self._support_vector_labels)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Computes the SVM prediction on the given features x.\n",
    "        \"\"\"\n",
    "        result = self._bias\n",
    "        print(x)\n",
    "        for z_i, x_i, y_i in zip(self._weights,\n",
    "                                 self._support_vectors,\n",
    "                                 self._support_vector_labels):\n",
    "            result += z_i * y_i * self._kernel(x_i, x)\n",
    "        return np.sign(result).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel(object):\n",
    "    \"\"\"Implements list of kernels from\n",
    "    http://en.wikipedia.org/wiki/Support_vector_machine\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        return lambda x, y: np.inner(x, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        return lambda x, y: \\\n",
    "            np.exp(-np.sqrt(la.norm(x-y) ** 2 / (2 * sigma ** 2)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _polykernel(dimension, offset):\n",
    "        return lambda x, y: (offset + np.inner(x, y)) ** dimension\n",
    "\n",
    "    @classmethod\n",
    "    def inhomogenous_polynomial(cls, dimension):\n",
    "        return cls._polykernel(dimension=dimension, offset=1.0)\n",
    "\n",
    "    @classmethod\n",
    "    def homogenous_polynomial(cls, dimension):\n",
    "        return cls._polykernel(dimension=dimension, offset=0.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(kappa, c):\n",
    "        return lambda x, y: np.tanh(kappa * np.dot(x, y) + c)\n",
    "\n",
    "    @staticmethod\n",
    "    def radial_basis(gamma=10):\n",
    "        return lambda x, y: np.exp(-gamma*la.norm(np.subtract(x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_bar, val_y):\n",
    "    \n",
    "    Error = 0\n",
    "    for i in range(len(y_bar)):\n",
    "        miss=y_bar[i-1]-val_y[i-1]\n",
    "        Error += miss\n",
    " \n",
    "    #print(\"Square Error: \", Error)\n",
    "    #print(\"E = \", sum(Error) / len(Error))  # 均方误差MSE\n",
    "    return Error / len(y_bar)\n",
    " \n",
    "def SVMResult(vardim, x, bound, dataset):\n",
    "    \n",
    "    X = dataset.loc[dataset['split'] == 'train'].iloc[:,0:-2].values\n",
    "    y = dataset.loc[dataset['split'] == 'train'].iloc[:,-2].values\n",
    "    val_X = dataset.loc[dataset['split'] == 'val'].iloc[:,0:-2].values\n",
    "    val_y = dataset.loc[dataset['split'] == 'val'].iloc[:,-2].values\n",
    "    c = x[0]\n",
    "    g = x[1]\n",
    "    #f = x[2]#四参数\n",
    "    rbf_kernel = Kernel.radial_basis(gamma=abs(g))\n",
    "    svm = SVMTrainer(rbf_kernel, abs(c))\n",
    "    predictor = svm.train(X, y)\n",
    "    y_bar = predictor.predict(val_X)\n",
    "    #predictval=clf.predict(val_X)\n",
    "    #print(predictval)\n",
    "    #return msefunc(predictval,val_y) \n",
    "    return score(y_bar, val_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W:\\conda\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a02b367e6648f5ab7f01253417909d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.5510e+02 -2.2264e+03  3e+03  4e-01  1e-15\n",
      " 1: -4.0179e+02 -6.1711e+02  2e+02  4e-03  6e-16\n",
      " 2: -5.6063e+02 -5.6933e+02  9e+00  9e-05  8e-16\n",
      " 3: -5.6219e+02 -5.6228e+02  9e-02  9e-07  6e-16\n",
      " 4: -5.6221e+02 -5.6221e+02  9e-04  9e-09  6e-16\n",
      " 5: -5.6221e+02 -5.6221e+02  9e-06  9e-11  8e-16\n",
      "Optimal solution found.\n",
      "[13.   76.   60.    0.    0.   32.8   0.18 41.  ]\n",
      "[  0.    127.     80.     37.    210.     36.3     0.804  23.   ]\n",
      "[  1.    103.     80.     11.     82.     19.4     0.491  22.   ]\n",
      "[ 2.    94.    76.    18.    66.    31.6    0.649 23.   ]\n",
      "[0.00e+00 1.39e+02 6.20e+01 1.70e+01 2.10e+02 2.21e+01 2.07e-01 2.10e+01]\n",
      "[  3.    108.     62.     24.      0.     26.      0.223  25.   ]\n",
      "[10.    92.    62.     0.     0.    25.9    0.167 31.   ]\n",
      "[  2.    100.     64.     23.      0.     29.7     0.368  21.   ]\n",
      "[  0.    141.     84.     26.      0.     32.4     0.433  22.   ]\n",
      "[  2.    146.     76.     35.    194.     38.2     0.329  29.   ]\n",
      "[  1.    164.     82.     43.     67.     32.8     0.341  50.   ]\n",
      "[ 1.    97.    70.    40.     0.    38.1    0.218 30.   ]\n",
      "[  2.    112.     75.     32.      0.     35.7     0.148  21.   ]\n",
      "[  0.    104.     64.     23.    116.     27.8     0.454  23.   ]\n",
      "[ 4.    90.    88.    47.    54.    37.7    0.362 29.   ]\n",
      "[ 6.    91.     0.     0.     0.    29.8    0.501 31.   ]\n",
      "[ 3.    99.    54.    19.    86.    25.6    0.154 24.   ]\n",
      "[ 2.   85.   65.    0.    0.   39.6   0.93 27.  ]\n",
      "[  0.    101.     62.      0.      0.     21.9     0.336  25.   ]\n",
      "[  0.   111.    65.     0.     0.    24.6    0.66  31.  ]\n",
      "[ 5.    96.    74.    18.    67.    33.6    0.997 43.   ]\n",
      "[  1.    135.     54.      0.      0.     26.7     0.687  62.   ]\n",
      "[  4.    103.     60.     33.    192.     24.      0.966  33.   ]\n",
      "[ 3.    61.    82.    28.     0.    34.4    0.243 46.   ]\n",
      "[  1.    124.     60.     32.      0.     35.8     0.514  21.   ]\n",
      "[ 8.    85.    55.    20.     0.    24.4    0.136 42.   ]\n",
      "[  1.    151.     60.      0.      0.     26.1     0.179  22.   ]\n",
      "[  3.    150.     76.      0.      0.     21.      0.207  37.   ]\n",
      "[ 7.    62.    78.     0.     0.    32.6    0.391 41.   ]\n",
      "[ 5.   95.   72.   33.    0.   37.7   0.37 27.  ]\n",
      "[  6.    98.    58.    33.   190.    34.     0.43  43.  ]\n",
      "[  2.    101.     58.     35.     90.     21.8     0.155  22.   ]\n",
      "[  7.    137.     90.     41.      0.     32.      0.391  39.   ]\n",
      "[  2.    120.     76.     37.    105.     39.7     0.215  29.   ]\n",
      "[  1.    108.     60.     46.    178.     35.5     0.415  24.   ]\n",
      "[ 1.    87.    78.    27.    32.    34.6    0.101 22.   ]\n",
      "[  0.     93.    100.     39.     72.     43.4     1.021  35.   ]\n",
      "[ 1.    91.    64.    24.     0.    29.2    0.192 21.   ]\n",
      "[ 1.    88.    78.    29.    76.    32.     0.365 29.   ]\n",
      "[ 1.    80.    74.    11.    60.    30.     0.527 22.   ]\n",
      "[ 1.    95.    66.    13.    38.    19.6    0.334 25.   ]\n",
      "[ 5.    73.    60.     0.     0.    26.8    0.268 27.   ]\n",
      "[  5.    123.     74.     40.     77.     34.1     0.269  28.   ]\n",
      "[ 2.    90.    60.     0.     0.    23.5    0.191 25.   ]\n",
      "[ 7.    94.    64.    25.    79.    33.3    0.738 41.   ]\n",
      "[  5.    132.     80.      0.      0.     26.8     0.186  69.   ]\n",
      "[ 12.    100.     84.     33.    105.     30.      0.488  46.   ]\n",
      "[  0.     94.     70.     27.    115.     43.5     0.347  21.   ]\n",
      "[  0.    106.     70.     37.    148.     39.4     0.605  22.   ]\n",
      "[ 1.    77.    56.    30.    56.    33.3    1.251 24.   ]\n",
      "[  0.    100.     88.     60.    110.     46.8     0.962  31.   ]\n",
      "[ 9.    57.    80.    37.     0.    32.8    0.096 41.   ]\n",
      "[ 3.    96.    78.    39.     0.    37.3    0.238 40.   ]\n",
      "[  8.    110.     76.      0.      0.     27.8     0.237  58.   ]\n",
      "[ 0.    93.    60.    25.    92.    28.7    0.532 22.   ]\n",
      "[  2.    108.     62.     32.     56.     25.2     0.128  21.   ]\n",
      "[  7.    102.     74.     40.    105.     37.2     0.204  45.   ]\n",
      "[  1.   119.    44.    47.    63.    35.5    0.28  25.  ]\n",
      "[  3.    180.     64.     25.     70.     34.      0.271  26.   ]\n",
      "[  0.    137.     84.     27.      0.     27.3     0.231  59.   ]\n",
      "[ 10.    115.     98.      0.      0.     24.      1.022  34.   ]\n",
      "[  2.  127.   58.   24.  275.   27.7   1.6  25. ]\n",
      "[ 4.    95.    60.    32.     0.    35.4    0.284 28.   ]\n",
      "[  3.     84.     68.     30.    106.     31.9     0.591  25.   ]\n",
      "[ 1.    88.    62.    24.    44.    29.9    0.422 23.   ]\n",
      "[ 6.    92.    92.     0.     0.    19.9    0.188 28.   ]\n",
      "[  0.    101.     64.     17.      0.     21.      0.252  21.   ]\n",
      "[  0.   126.    84.    29.   215.    30.7    0.52  24.  ]\n",
      "[ 1.    85.    66.    29.     0.    26.6    0.351 31.   ]\n",
      "[  2.    112.     86.     42.    160.     38.4     0.246  28.   ]\n",
      "[ 13.    153.     88.     37.    140.     40.6     1.174  39.   ]\n",
      "[1.00e+00 1.28e+02 8.20e+01 1.70e+01 1.83e+02 2.75e+01 1.15e-01 2.20e+01]\n",
      "[ 2.    96.    68.    13.    49.    21.1    0.647 26.   ]\n",
      "[ 2.    90.    80.    14.    55.    24.4    0.249 24.   ]\n",
      "[  2.    109.     92.      0.      0.     42.7     0.845  54.   ]\n",
      "[2.00e+00 1.25e+02 6.00e+01 2.00e+01 1.40e+02 3.38e+01 8.80e-02 3.10e+01]\n",
      "[  1.    111.     94.      0.      0.     32.8     0.265  45.   ]\n",
      "[  5.    128.     80.      0.      0.     34.6     0.144  45.   ]\n",
      "[ 1.    92.    62.    25.    41.    19.5    0.482 25.   ]\n",
      "[ 3.    82.    70.     0.     0.    21.1    0.389 25.   ]\n",
      "[ 2.    95.    54.    14.    88.    26.1    0.748 22.   ]\n",
      "[  1.    143.     84.     23.    310.     42.4     1.076  22.   ]\n",
      "[  2.    119.      0.      0.      0.     19.6     0.832  72.   ]\n",
      "[  1.    143.     74.     22.     61.     26.2     0.256  21.   ]\n",
      "[ 0.    94.     0.     0.     0.     0.     0.256 25.   ]\n",
      "[  5.    147.     75.      0.      0.     29.9     0.434  28.   ]\n",
      "[  4.    132.     86.     31.      0.     28.      0.419  63.   ]\n",
      "[  3.    122.     78.      0.      0.     23.      0.254  40.   ]\n",
      "[  2.    129.      0.      0.      0.     38.5     0.304  41.   ]\n",
      "[  5.    104.     74.      0.      0.     28.8     0.153  48.   ]\n",
      "[  3.  102.   44.   20.   94.   30.8   0.4  26. ]\n",
      "[  0.    165.     76.     43.    255.     47.9     0.259  26.   ]\n",
      "[ 2.    99.     0.     0.     0.    22.2    0.108 23.   ]\n",
      "[  2.    175.     88.      0.      0.     22.9     0.326  22.   ]\n",
      "[  0.    134.     58.     20.    291.     26.4     0.352  21.   ]\n",
      "[ 2.    99.    70.    16.    44.    20.4    0.235 27.   ]\n",
      "[  3.    111.     56.     39.      0.     30.1     0.557  30.   ]\n",
      "[  7.    133.     84.      0.      0.     40.2     0.696  37.   ]\n",
      "[4.00e+00 1.31e+02 6.80e+01 2.10e+01 1.66e+02 3.31e+01 1.60e-01 2.80e+01]\n",
      "[ 10.     68.    106.     23.     49.     35.5     0.285  47.   ]\n",
      "[  1.    107.     68.     19.      0.     26.5     0.165  24.   ]\n",
      "[  9.    154.     78.     30.    100.     30.9     0.164  45.   ]\n",
      "[6.00e+00 9.20e+01 6.20e+01 3.20e+01 1.26e+02 3.20e+01 8.50e-02 4.60e+01]\n",
      "[  2.    106.     56.     27.    165.     29.      0.426  22.   ]\n",
      "[ 4.    95.    70.    32.     0.    32.1    0.612 24.   ]\n",
      "[  4.    118.     70.      0.      0.     44.5     0.904  26.   ]\n",
      "[  5.    103.    108.     37.      0.     39.2     0.305  65.   ]\n",
      "[  1.    116.     78.     29.    180.     36.1     0.496  25.   ]\n",
      "[  0.    117.      0.      0.      0.     33.8     0.932  44.   ]\n",
      "[  3.    148.     66.     25.      0.     32.5     0.256  22.   ]\n",
      "[  1.     96.    122.      0.      0.     22.4     0.207  27.   ]\n",
      "[ 0.   95.   80.   45.   92.   36.5   0.33 26.  ]\n",
      "[  2.    130.     96.      0.      0.     22.6     0.268  21.   ]\n",
      "[ 1.    95.    74.    21.    73.    25.9    0.673 36.   ]\n",
      "[  8.    120.     78.      0.      0.     25.      0.409  64.   ]\n",
      "[  0.    102.     64.     46.     78.     40.6     0.496  21.   ]\n",
      "[  1.    125.     70.     24.    110.     24.3     0.221  25.   ]\n",
      "[ 1.    93.    56.    11.     0.    22.5    0.417 22.   ]\n",
      "[ 11.   127.   106.     0.     0.    39.     0.19  51.  ]\n",
      "[2.00e+00 1.57e+02 7.40e+01 3.50e+01 4.40e+02 3.94e+01 1.34e-01 3.00e+01]\n",
      "[ 0.    67.    76.     0.     0.    45.3    0.194 46.   ]\n",
      "[  1.    139.     62.     41.    480.     40.7     0.536  21.   ]\n",
      "[ 0.    93.    60.     0.     0.    35.3    0.263 25.   ]\n",
      "[  3.    126.     88.     41.    235.     39.3     0.704  27.   ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.    114.     88.      0.      0.     27.8     0.247  66.   ]\n",
      "[  0.    129.     80.      0.      0.     31.2     0.703  29.   ]\n",
      "[  6.    125.     68.     30.    120.     30.      0.464  32.   ]\n",
      "[  4.    120.     68.      0.      0.     29.6     0.709  34.   ]\n",
      "[11.  85.  74.   0.   0.  30.1  0.3 35. ]\n",
      "[  2.   105.    75.     0.     0.    23.3    0.56  53.  ]\n",
      "[  6.   111.    64.    39.     0.    34.2    0.26  24.  ]\n",
      "[  9.   106.    52.     0.     0.    31.2    0.38  42.  ]\n",
      "[ 2.    68.    70.    32.    66.    25.     0.187 25.   ]\n",
      "[  8.    126.     74.     38.     75.     25.9     0.162  39.   ]\n",
      "[  1.     84.     64.     23.    115.     36.9     0.471  28.   ]\n",
      "[ 9.    89.    62.     0.     0.    22.5    0.142 33.   ]\n",
      "[  3.    124.     80.     33.    130.     33.2     0.305  26.   ]\n",
      "[ 1.   95.   60.   18.   58.   23.9   0.26 22.  ]\n",
      "[ 1.    90.    68.     8.     0.    24.5    1.138 36.   ]\n",
      "[ 0.    99.     0.     0.     0.    25.     0.253 22.   ]\n",
      "[  5.    111.     72.     28.      0.     23.9     0.407  27.   ]\n",
      "[  4.    129.     60.     12.    231.     27.5     0.527  31.   ]\n",
      "[ 6.    85.    78.     0.     0.    31.2    0.382 42.   ]\n",
      "[  6.    123.     72.     45.    230.     33.6     0.733  34.   ]\n",
      "[  5.    158.     70.      0.      0.     29.8     0.207  63.   ]\n",
      "[  0.    125.     96.      0.      0.     22.5     0.262  21.   ]\n",
      "[  3.     96.     56.     34.    115.     24.7     0.944  39.   ]\n",
      "[ 1.    96.    64.    27.    87.    33.2    0.289 21.   ]\n",
      "[[  5.    137.    108.    ...  48.8     0.227  37.   ]\n",
      " [  5.    108.     72.    ...  36.1     0.263  33.   ]\n",
      " [  4.    154.     72.    ...  31.3     0.338  37.   ]\n",
      " ...\n",
      " [  6.    125.     78.    ...  27.6     0.565  49.   ]\n",
      " [  4.    146.     92.    ...  31.2     0.539  61.   ]\n",
      " [  0.     91.     80.    ...  32.4     0.601  27.   ]]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-a4db08d0a1bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mga\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#调高种群大小， 调低生存比 //又调回来了\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mga\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-1a9486b4b316>\u001b[0m in \u001b[0;36msolve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;31m#best = np.max(self.fitness)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m#bestIndex = np.argmax(self.fitness)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-1a9486b4b316>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mfitness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculateFitness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[1;31m#print(\"###/n\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-f549a64e9c52>\u001b[0m in \u001b[0;36mcalculateFitness\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#print(self.chrom)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         self.fitness = SVMResult(\n\u001b[1;32m---> 26\u001b[1;33m             self.vardim, self.chrom, self.bound, dataset)\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprint_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-95-6c82e14642bd>\u001b[0m in \u001b[0;36mSVMResult\u001b[1;34m(vardim, x, bound, dataset)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m#print(predictval)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m#return msefunc(predictval,val_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_bar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-95-6c82e14642bd>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(y_bar, val_y)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mError\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_bar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mmiss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_bar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mError\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "bound = np.array([[0,0],[20, 1]])\n",
    "ga = GA(50, 2, bound, 50, [0.5, 0.1, 0.5], 7)#调高种群大小， 调低生存比 //又调回来了\n",
    "ga.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ga.trace[t, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W:\\conda\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "W:\\conda\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "def creat_dataset1(df):\n",
    "        df = shuffle(df)\n",
    "        size = len(df)\n",
    "        df['split'] = 0\n",
    "        df.iloc[0:math.ceil(0.3*size),-1] = 'train'\n",
    "        df.iloc[math.ceil(0.7*size):, -1] = 'val'\n",
    "        #df.iloc[math.ceil(0.85*size):size, -1] = 'val'\n",
    "\n",
    "        return df\n",
    "dataset = creat_dataset1(pd.read_csv(\"diabetes.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = np.tile([[0], [1]], 2) \n",
    "bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6565217391304348\n"
     ]
    }
   ],
   "source": [
    "def SVMResult1(dataset):\n",
    "    X = dataset.loc[dataset['split'] == 'train'].iloc[:,0:-2].values\n",
    "    y = dataset.loc[dataset['split'] == 'train'].iloc[:,-2].values\n",
    "    val_X = dataset.loc[dataset['split'] == 'val'].iloc[:,0:-2].values\n",
    "    val_y = dataset.loc[dataset['split'] == 'val'].iloc[:,-2].values\n",
    "    \n",
    "    clf = svm.SVC(C=1.21619432, gamma=0.00701896, kernel='rbf')\n",
    "    clf.fit(X, y)\n",
    "    #predictval=clf.predict(val_X)\n",
    "    #print(predictval)\n",
    "    #return msefunc(predictval,val_y) \n",
    "    return clf.score(val_X, val_y)\n",
    "print(SVMResult1(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,45,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[[2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt.solvers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
